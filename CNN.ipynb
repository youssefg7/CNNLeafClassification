{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torchvision import transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 990 images in the dataset\n",
      "There are 99 labels in the dataset\n",
      "(990,)\n"
     ]
    }
   ],
   "source": [
    "trainSet = pd.read_csv('Dataset/train.csv')\n",
    "classes = trainSet[['id','species']].copy()\n",
    "classes['id'] = classes['id'].astype(str)\n",
    "classes['label'] = LabelEncoder().fit_transform(classes['species'])\n",
    "\n",
    "image_folder = 'Dataset/images/'\n",
    "imgs = []\n",
    "labels = []\n",
    "for i in sorted(os.listdir(image_folder)):\n",
    "    id = i.split('.')[0]\n",
    "    if id in classes['id'].values:\n",
    "        labels.append(classes[classes['id'] == id]['label'].values[0])\n",
    "        image = Image.open(os.path.join(image_folder, i)).convert('1')\n",
    "        imgs.append(image)\n",
    "\n",
    "print(f\"There are {len(imgs)} images in the dataset\")\n",
    "print(f\"There are {len(np.unique(labels))} labels in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 792 images in the training set\n",
      "There are 198 images in the test set\n",
      "There are 99 labels in the training set\n",
      "There are 99 labels in the test set\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(imgs, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "print(f\"There are {len(X_train)} images in the training set\")\n",
    "print(f\"There are {len(X_test)} images in the test set\")\n",
    "print(f\"There are {len(np.unique(y_train))} labels in the training set\")\n",
    "print(f\"There are {len(np.unique(y_test))} labels in the test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, imgs, labels):\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        self.transform = A.Compose([\n",
    "            transforms.Resize((224, 224)),  # Adjust the size as needed\n",
    "            ToTensorV2()])\n",
    "        \n",
    "        data = self.transform(image = np.array(self.imgs[index],dtype=np.float32), \n",
    "                              label = np.array(self.labels[index],dtype=np.float32))\n",
    "        img = data['image']\n",
    "        label = data['label']\n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "trainDataSet = CustomDataSet(X_train, y_train)\n",
    "testDataSet = CustomDataSet(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNmodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNmodel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1) \n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.fc1 = nn.Linear(256, 512)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 99)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.pool3(self.relu3(self.conv3(x)))\n",
    "        x = self.pool4(self.relu4(self.conv4(x)))\n",
    "\n",
    "        x = self.global_avg_pool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu5(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'image'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Volumes/D/Uni/Semester 9/CSE485 Deep Learning/Project/CNN.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/D/Uni/Semester%209/CSE485%20Deep%20Learning/Project/CNN.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/D/Uni/Semester%209/CSE485%20Deep%20Learning/Project/CNN.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Volumes/D/Uni/Semester%209/CSE485%20Deep%20Learning/Project/CNN.ipynb#W6sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m images, labels \u001b[39min\u001b[39;00m trainDataLoader:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/D/Uni/Semester%209/CSE485%20Deep%20Learning/Project/CNN.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/D/Uni/Semester%209/CSE485%20Deep%20Learning/Project/CNN.ipynb#W6sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(images)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/Volumes/D/Uni/Semester 9/CSE485 Deep Learning/Project/CNN.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/D/Uni/Semester%209/CSE485%20Deep%20Learning/Project/CNN.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/D/Uni/Semester%209/CSE485%20Deep%20Learning/Project/CNN.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39m=\u001b[39m A\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/D/Uni/Semester%209/CSE485%20Deep%20Learning/Project/CNN.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         transforms\u001b[39m.\u001b[39mResize((\u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m)),  \u001b[39m# Adjust the size as needed\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/D/Uni/Semester%209/CSE485%20Deep%20Learning/Project/CNN.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         ToTensorV2()])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Volumes/D/Uni/Semester%209/CSE485%20Deep%20Learning/Project/CNN.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(image \u001b[39m=\u001b[39;49m np\u001b[39m.\u001b[39;49marray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimgs[index],dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat32), \n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/D/Uni/Semester%209/CSE485%20Deep%20Learning/Project/CNN.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                           label \u001b[39m=\u001b[39;49m np\u001b[39m.\u001b[39;49marray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabels[index],dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat32))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/D/Uni/Semester%209/CSE485%20Deep%20Learning/Project/CNN.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     img \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/D/Uni/Semester%209/CSE485%20Deep%20Learning/Project/CNN.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     label \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/albumentations/core/composition.py:210\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[1;32m    207\u001b[0m     p\u001b[39m.\u001b[39mpreprocess(data)\n\u001b[1;32m    209\u001b[0m \u001b[39mfor\u001b[39;00m idx, t \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(transforms):\n\u001b[0;32m--> 210\u001b[0m     data \u001b[39m=\u001b[39m t(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdata)\n\u001b[1;32m    212\u001b[0m     \u001b[39mif\u001b[39;00m check_each_transform:\n\u001b[1;32m    213\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_data_post_transform(data)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'image'"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "trainDataLoader = torch.utils.data.DataLoader(trainDataSet, batch_size=batch_size)\n",
    "testDataLoader = torch.utils.data.DataLoader(testDataSet, batch_size=batch_size)\n",
    "\n",
    "min_loss_epoch = 0\n",
    "min_loss_value = -1\n",
    "best_model_weights_paths = {}\n",
    "\n",
    "best_val_loss = float('inf')  # Initialize with a large value\n",
    "best_epoch = -1\n",
    "best_model_weights = None\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = CNNmodel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in trainDataLoader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        print(f\"Epoch: {epoch+1}/{num_epochs}  Loss: {loss.item():.4f}\")\n",
    "    train_loss = running_loss / len(trainDataLoader)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()  \n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testDataLoader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    val_loss = 1-accuracy\n",
    "    val_losses.append(accuracy)\n",
    "\n",
    "\n",
    "best_epoch = val_losses.index(min(val_losses))\n",
    "min_loss_epoch = best_epoch\n",
    "min_loss_value = f'{min(val_losses):.4f}'\n",
    "print(f\"Min Train Loss: {min(train_losses)} at Epoch {train_losses.index(min(train_losses))}  Min Val Loss: {min_loss_value[criterion]} at Epoch {best_epoch}\")\n",
    "\n",
    "\n",
    "# Save the best model's weights\n",
    "torch.save(model, f\"best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
